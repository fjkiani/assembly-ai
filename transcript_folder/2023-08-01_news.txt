Hey, Ian. How's it going? Jesse from AssemblyAI. Hey, going well? Going well. How about good, good. Appreciate you taking the time today. We could jump right into it, but I understand that you're a sales intelligence platform. Really excited to chat with you. We work with a ton of companies in that space, jiminy, Fathom, just to name a couple. There are a lot of potential areas where I think we could partner on, but excited to hear what's top of mind for you and I'll kick it back over to you and we can take it from yeah, yeah. For really, really appreciate know we've been kind of experimenting with a couple different third party providers for transcription, mainly like Google and AWS, leveraging their capabilities. Super expensive, and honestly, the accuracy is not that to. And we've been using Google and AWS in kind of limited capacity. We've also built some of these speech recognition models in house, but it's getting super difficult for us to kind of upkeep and maintain. So yeah, just kind of wanted to explore and I've seen a lot of press about assembly, seen you guys popping up around pretty much everywhere. So curious to learn more, maybe see a little bit how you're different transcriptions, top of mind and then also obviously all this stuff with chat GPT. Curious how this comes into play. There's a lot of stuff that we're getting asked for to build by our customers and curious kind of how you guys play in that space too. Awesome, really appreciate all that. It's definitely helpful context. Before we get into assembly and our pitch, I'm just going to have a few questions for you. That sounds good, but you had mentioned that you're currently using Google and AWS. Do you mind me asking why both providers, what are the use cases for each of them? Yeah, so we were using Google for real time and then AWS for asynchronous like post call transcription and honestly we just kind of put it together kind of on an ad hoc basis as our customers would ask for it. And we had some credits from AWS and some credits from Google that we were kind of leveraging there too. But honestly, not a whole lot of rhyme and reason. Those are just kind of where we defaulted to. Okay, sounds good. And you had mentioned they're expensive, so it seems like price is a pretty big sticking point. Outside of price, what are other evaluation criteria that you guys are looking for in a provider? Yeah, so pricing is obviously super important so that we can kind of protect our margins and give good deals to our customers, help us close deals there. Accuracy is obviously super important. Ease of implementation. We have kind of a limited team that we're working with internally and so really just kind of trying to take stuff off of our internal team shoulders, seeing what we can outsource and then I think additionally something that's interesting is like, what else past transcription can we do? We've built some stuff in house, but pretty lightweight, like sentiment analysis, keyword matching, entity detection. We've built some of that stuff in house, but yeah, really kind of excited and curious to learn what else we can leverage. So I'd say those are like the three main evaluation criteria points. Okay, excellent. That's definitely helpful. I mean, yeah, like other companies we work with in this space, not only are they leveraging the core transcription, but there are those downstream models like sentiment and summarization auto chapters to get a better sense of what's happening within those calls, to identify and eliminate risks for those reps, et cetera. And it all starts with having an accurate transcription and typically what we hear is going out and trying to build something like this in house is not only really time consuming, but it's expensive. Are you experiencing similar? Does that sound familiar? Does that sound like something you guys are experiencing? Yeah, I think so. I think it's just like kind of balancing asks from our customers with what we can actually get into production and then how much would our customers be willing to pay? Again, these are a lot of unknowns that we don't know and it's kind of risky when we look at trying to see what kind of products can we ship, what will our customers pay for them? We'd prefer to know for sure that there's a subset of our users that want this feature, this is how much they pay for, but obviously you're not going to get that every time. So I think it's trying to weigh out that kind of balance of internal bandwidth, our internal cost, and then what's the real demand from our customers. Okay, awesome. I think you're thinking about this the exact right way and I'm confident that we could add a lot of value and help your team out a lot. Lastly, before we jump into a little bit about assembly, because I know price is top of mind for you, our pricing is volume based. There are economies of scale here. So do you have a sense of what your current volume is right now? Like how many hours a month would you be submitting through our API? Yeah, so I would need to go back to our team to get some concrete numbers there, but I'm happy to share those after the call. But yeah, just high level. I think at full scale we're probably around like 25,000 hours a month, I would say. But then the active users right now that are transcribing our platform is probably close to like 5000 hours a month. But let me go back to the team, I'll solidify those numbers and I can share those with you after this. Okay, awesome, I appreciate it. Thank you for bearing with me. I'm sorry to bombard you with the questions. A little bit of background on assembly to give you a sense of who we are and where we sit in the market. We were founded a few years ago and we've raised about $63 million to date with our Series A and Series B coming last year. We've gone out and hired top researchers from companies like Meta, Google, AWS, and we're backed by venture capitalists like Insight, Excel, and Y Combinator. In addition to that, we work with Fortune 500 companies all the way to startups. Some notable ones would be Spotify, Jiminy, CallRail, Fathom, right, the list goes on and on. And I'm happy to send this over to you as well. But where I think we sit in the market. And what I think would be really relevant for you as a team who's building this out or thinking about building this out in house, is that this tech is really moving quickly, and there's more advancements coming out each and every day out of these research labs, like DeepMind and OpenAI. Right? Like, we've all seen the news with Chat GPT, which is exciting. And what assembly is doing is that we're monitoring these updates on a weekly basis, essentially. And we're seeing the research that comes out of these labs, and we're tweaking that, and we're making those models production ready, scalable and secure so that product teams like yours can quickly launch products, get feedback from their customers, bring that back to us, and then you'll have the ability to quickly iterate. And we'll be an extension of your team as your AI department essentially when it comes to audio data. So that's just a little bit of background on where we sit in the market. Any questions so far? Yeah, no, I think that's interesting. So you guys are basically taking the models that OpenAI creates and then just making tweaks to those models and wrapping them up and then repurposing it or what exactly do you mean by that? Yeah, there's more that goes into it, but right, there's thousands and thousands of models out there. But it's really hard for product teams to digest all of that and make it specific to their use case based on the data sets that they need. And these models that are getting released and the research that's getting released, they're not production ready. There's a lot of work that needs to go into that to make it secure, scalable, and production ready. And that's kind of where we come in to have one single API that you can call into to get everything that you need in an output. So our bread and butter is transcription accuracy. That's what we were founded on. And we've really doubled down on being the most accurate provider in the space. And we actually just released our conformer one model based on this research that came out of DeepMind that said that all ASR providers, including ourselves, were massively under training these models. And in reality, we need to be training on a much larger data set. So that's what we did. And we're now training our model on 650,000 hours, which is going to be a step change improvement when it comes to accuracy. But why that's so important is because when you start thinking about those downstream tasks, like sentiment, summarization, PII, redaction, it all stems back from having a really accurate transcript, and it's a garbage in, garbage out model. So that's kind of why we've doubled down on the transcription area, and it's hard for teams to replicate that. Again, really time consuming, really expensive. Got you. And what is your guys'average accuracy? What do you say? So it depends based on real time or asynchronous depends on the language, but for English, which is going to be our highest, and what you're transcribing mostly, we typically are in the mid to high 90s. It's like near human like transcription. Again, these models are getting smarter, and we're going to be launching our conformer two down the road, and there's going to be more advancements that are just coming on a quarterly basis, essentially just getting more and more accurate. Got you. Yeah, super exciting. Cool. So I mean, based on everything that we went over today, what are your thoughts, what are your feedback? Do you see this potentially adding and fitting into your current roadmap? Yeah, I think so. I think I kind of got tasked with scoping this out and seeing what's out there with third party providers. Again, these were asks that we knew we had to do, and so we just went with the quickest route that we saw to production probably about two years ago with Google and AWS and haven't really paid much attention to it since then, but now we're looking to double down. Obviously there's been advancements in the space, transcription has become much more accurate, so we can trust it more, and we really want to build a lot of these downstream models, too. So I think that we'd definitely be interested in testing. I think we're obviously super curious about pricing. How should we go about testing? Should I just go sign up on your site or is there something there? Yeah, you let me write to my next point. So typically what we like to recommend is getting some audio files from your end so we could run a benchmark report. We'll have those human transcribed and we'll match that up to our transcription to come up with a word error rate to show you where we stack up against other people in the space, like AWS, Google, Deepgram, et cetera. We could also put together a demo environment that I'll send over an example for, for Jiminy to show you what those downstream tasks look like so your product team doesn't have to go build that in house. And this should be pretty easy to go pitch internally, and then we can reconnect to review the benchmark results, review the demo environment, and also have a deeper discussion around pricing as well. Does that all sound good to yeah. I'd be interested to test the API myself as well, along with some of my engineers. But I think also would be interested to send you some files, especially see the output of the demo app. That sounds interesting. We'd probably need to sign an NDA in order for that to happen, especially if we're sending customer examples, which I would want to run it on customer examples. So, yeah, we'd probably want to get an NDA in place, but we can probably handle that via email. All right, sounds good. Should we set a time to reconnect sometime next week to review the benchmark, review the demo environment pending the NDA and us getting the files from you? Sounds great. Yeah, this time next week would be perfect. All right, excellent. Really appreciate the time, Ian. Looking forward to reconnecting next week. Likewise. Thanks, Jesse. All right, let me stop this.